# Compositional Generalization and Natural Language Variation: Can a Semantic Parsing Approach Handle Both?

This directory contains code related to the paper "Compositional Generalization
and Natural Language Variation: Can a Semantic Parsing Approach Handle Both?"
(Peter Shaw, Ming-Wei Chang, Panupong Pasupat, Kristina Toutanova).

The current version of this library contains code for reproducing the dataset
splits used in the paper.

[TOC]

## Datasets

Below are instructions for reproducing the dataset splits used in the paper.

We use a standard TSV format for representing all splits, where each
line corresponds to an example and is formatted as:

`<source>\t<target>\n`

Where `<source>` is the input string and `<target>` is the output string.

### SCAN

The "add primitive" and "length" splits, as well as the original dataset,
are available here:

https://github.com/brendenlake/SCAN

Instructions to produce the SCAN MCD splits are here:

https://github.com/google-research/google-research/tree/master/cfq#scan-mcd-splits

The SCAN files can be converted to our TSV format by using the
`tasks/scan/convert_to_tsv.py` script for files in the original dataset format,
and `tasks/scan/join_txt_to_tsv.py` to join the input and output txt files
generated for the MCD splits.

### GeoQuery

You can learn more about the GeoQuery dataset here:
https://www.cs.utexas.edu/users/ml/nldata/geoquery.html

You can download the GeoQuery corpus with FunQL annotated expressions here:
http://www.cs.utexas.edu/~ml/wasp/geo-funql/corpus.xml

You will also need to download the geobase file which is used to identify
entities to replace with placeholders:
[ftp://ftp.cs.utexas.edu/pub/mooney/nl-ilp-data/geosystem/geobase](ftp://ftp.cs.utexas.edu/pub/mooney/nl-ilp-data/geosystem/geobase)

You can then generate the dataset in TSV format using the
`tasks/geoquery/write_dataset.py` script.

Example usage:

```
nqg/tasks/geoquery/write_dataset.py \
--corpus=/path/to/.../geoquery.xml \
--geobase=/path/to/.../geobase \
--output=/path/to/.../dataset.tsv
```


You can then reproduce the four splits used in the paper using the
`tasks/split_dataset.py` script, with the output of `write_dataset.py`
as `--input` and a file in `tasks/geoquery/splits` as `--split`.


### Spider

The Spider dataset can be downloaded from this location:
https://yale-lily.github.io/spider

Below, we will assume that the environment variable `SPIDER_DIR` points to a
directory that contains the Spider dataset.

In the paper we use various splits of a dataset we refer to as Spider-SSP which
contains all examples in the original Spider training set for databases with
at least 50 examples. This set of databases can be determined using the
`tasks/spider/print_database_counts.py` script, but is also hardcoded in
`tasks/spider/database_constants.py`.

You can generate the Spider-SSP dataset in TSV format using the
`tasks/spider/write_dataset.py` script with
`--examples=${SPIDER_DIR}/data/train_spider.json`.


When using T5, we append a serialized database schema to the input string.
This can be accomplished using the `tasks/spider/append_schema.py` script with
`--input` set to the TSV generated by `tasks/spider/write_dataset.py` and
`--tables=${SPIDER_DIR}/data/tables.json`.


You can then reproduce the four splits used in the paper using the
`tasks/split_dataset.py` script, with the output of `write_dataset.py` or
`append_schema.py` as `--input` and a file in `tasks/spider/splits` as
`--split`.


For evaluation, you will need to download the Spider `evaluation.py` script
from: https://github.com/taoyds/spider.

You can then use `tasks/spider/generate_gold.py` to generate an input file
for the `--gold` flag of `evaluation.py`. The `--preds` flag should point to a
txt file of generated predictions.

Note that for T5, you will need to run `tasks/spider/restore_oov.py` to
post-process generated predictions.

## Approaches

### T5

Instructions for fine-tuning T5 given a dataset in the TSV format described
above are here:

https://github.com/google-research/text-to-text-transfer-transformer#using-a-tsv-file-directly

This document also contains instructions for generating predictions:

https://github.com/google-research/text-to-text-transfer-transformer#decode

To create a txt file with inputs to generate test predictions from a test tsv
file, you can use the `tasks/strip_targets.py` script. For datasets using
simple exact match (GeoQuery and SCAN), you can then compare these predictions
with the targets provided by a TSV file for a given test split using the script
`tasks/compare_predictions.py`.


